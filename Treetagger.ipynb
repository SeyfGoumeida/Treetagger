{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assets : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n",
      "------------------\n",
      "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'cb04', 'cb05', 'cb06', 'cb07', 'cb08', 'cb09', 'cb10', 'cb11', 'cb12', 'cb13', 'cb14', 'cb15', 'cb16', 'cb17', 'cb18', 'cb19', 'cb20', 'cb21', 'cb22', 'cb23', 'cb24', 'cb25', 'cb26', 'cb27', 'cc01', 'cc02', 'cc03', 'cc04', 'cc05', 'cc06', 'cc07', 'cc08', 'cc09', 'cc10', 'cc11', 'cc12', 'cc13', 'cc14', 'cc15', 'cc16', 'cc17', 'cd01', 'cd02', 'cd03', 'cd04', 'cd05', 'cd06', 'cd07', 'cd08', 'cd09', 'cd10', 'cd11', 'cd12', 'cd13', 'cd14', 'cd15', 'cd16', 'cd17', 'ce01', 'ce02', 'ce03', 'ce04', 'ce05', 'ce06', 'ce07', 'ce08', 'ce09', 'ce10', 'ce11', 'ce12', 'ce13', 'ce14', 'ce15', 'ce16', 'ce17', 'ce18', 'ce19', 'ce20', 'ce21', 'ce22', 'ce23', 'ce24', 'ce25', 'ce26', 'ce27', 'ce28', 'ce29', 'ce30', 'ce31', 'ce32', 'ce33', 'ce34', 'ce35', 'ce36', 'cf01', 'cf02', 'cf03', 'cf04', 'cf05', 'cf06', 'cf07', 'cf08', 'cf09', 'cf10', 'cf11', 'cf12', 'cf13', 'cf14', 'cf15', 'cf16', 'cf17', 'cf18', 'cf19', 'cf20', 'cf21', 'cf22', 'cf23', 'cf24', 'cf25', 'cf26', 'cf27', 'cf28', 'cf29', 'cf30', 'cf31', 'cf32', 'cf33', 'cf34', 'cf35', 'cf36', 'cf37', 'cf38', 'cf39', 'cf40', 'cf41', 'cf42', 'cf43', 'cf44', 'cf45', 'cf46', 'cf47', 'cf48', 'cg01', 'cg02', 'cg03', 'cg04', 'cg05', 'cg06', 'cg07', 'cg08', 'cg09', 'cg10', 'cg11', 'cg12', 'cg13', 'cg14', 'cg15', 'cg16', 'cg17', 'cg18', 'cg19', 'cg20', 'cg21', 'cg22', 'cg23', 'cg24', 'cg25', 'cg26', 'cg27', 'cg28', 'cg29', 'cg30', 'cg31', 'cg32', 'cg33', 'cg34', 'cg35', 'cg36', 'cg37', 'cg38', 'cg39', 'cg40', 'cg41', 'cg42', 'cg43', 'cg44', 'cg45', 'cg46', 'cg47', 'cg48', 'cg49', 'cg50', 'cg51', 'cg52', 'cg53', 'cg54', 'cg55', 'cg56', 'cg57', 'cg58', 'cg59', 'cg60', 'cg61', 'cg62', 'cg63', 'cg64', 'cg65', 'cg66', 'cg67', 'cg68', 'cg69', 'cg70', 'cg71', 'cg72', 'cg73', 'cg74', 'cg75', 'ch01', 'ch02', 'ch03', 'ch04', 'ch05', 'ch06', 'ch07', 'ch08', 'ch09', 'ch10', 'ch11', 'ch12', 'ch13', 'ch14', 'ch15', 'ch16', 'ch17', 'ch18', 'ch19', 'ch20', 'ch21', 'ch22', 'ch23', 'ch24', 'ch25', 'ch26', 'ch27', 'ch28', 'ch29', 'ch30', 'cj01', 'cj02', 'cj03', 'cj04', 'cj05', 'cj06', 'cj07', 'cj08', 'cj09', 'cj10', 'cj11', 'cj12', 'cj13', 'cj14', 'cj15', 'cj16', 'cj17', 'cj18', 'cj19', 'cj20', 'cj21', 'cj22', 'cj23', 'cj24', 'cj25', 'cj26', 'cj27', 'cj28', 'cj29', 'cj30', 'cj31', 'cj32', 'cj33', 'cj34', 'cj35', 'cj36', 'cj37', 'cj38', 'cj39', 'cj40', 'cj41', 'cj42', 'cj43', 'cj44', 'cj45', 'cj46', 'cj47', 'cj48', 'cj49', 'cj50', 'cj51', 'cj52', 'cj53', 'cj54', 'cj55', 'cj56', 'cj57', 'cj58', 'cj59', 'cj60', 'cj61', 'cj62', 'cj63', 'cj64', 'cj65', 'cj66', 'cj67', 'cj68', 'cj69', 'cj70', 'cj71', 'cj72', 'cj73', 'cj74', 'cj75', 'cj76', 'cj77', 'cj78', 'cj79', 'cj80', 'ck01', 'ck02', 'ck03', 'ck04', 'ck05', 'ck06', 'ck07', 'ck08', 'ck09', 'ck10', 'ck11', 'ck12', 'ck13', 'ck14', 'ck15', 'ck16', 'ck17', 'ck18', 'ck19', 'ck20', 'ck21', 'ck22', 'ck23', 'ck24', 'ck25', 'ck26', 'ck27', 'ck28', 'ck29', 'cl01', 'cl02', 'cl03', 'cl04', 'cl05', 'cl06', 'cl07', 'cl08', 'cl09', 'cl10', 'cl11', 'cl12', 'cl13', 'cl14', 'cl15', 'cl16', 'cl17', 'cl18', 'cl19', 'cl20', 'cl21', 'cl22', 'cl23', 'cl24', 'cm01', 'cm02', 'cm03', 'cm04', 'cm05', 'cm06', 'cn01', 'cn02', 'cn03', 'cn04', 'cn05', 'cn06', 'cn07', 'cn08', 'cn09', 'cn10', 'cn11', 'cn12', 'cn13', 'cn14', 'cn15', 'cn16', 'cn17', 'cn18', 'cn19', 'cn20', 'cn21', 'cn22', 'cn23', 'cn24', 'cn25', 'cn26', 'cn27', 'cn28', 'cn29', 'cp01', 'cp02', 'cp03', 'cp04', 'cp05', 'cp06', 'cp07', 'cp08', 'cp09', 'cp10', 'cp11', 'cp12', 'cp13', 'cp14', 'cp15', 'cp16', 'cp17', 'cp18', 'cp19', 'cp20', 'cp21', 'cp22', 'cp23', 'cp24', 'cp25', 'cp26', 'cp27', 'cp28', 'cp29', 'cr01', 'cr02', 'cr03', 'cr04', 'cr05', 'cr06', 'cr07', 'cr08', 'cr09']\n",
      "------------------\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
      "------------------\n",
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "------------------\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(brown.tagged_words())\n",
    "print(\"------------------\")\n",
    "print(brown.fileids())\n",
    "print(\"------------------\")\n",
    "brown.readme()\n",
    "print(brown.words())\n",
    "print(\"------------------\")\n",
    "print(brown.categories())\n",
    "print(\"------------------\")\n",
    "print(brown.words(categories='news'))\n",
    "brown.sents(categories=['news', 'editorial', 'reviews'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## work on the first 30 fileids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = brown.fileids()\n",
    "f = f[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the phrases with the good sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of phrases with good sequence : 17\n",
      "the list of indexs of the phrase in phrases   [137, 324, 394, 437, 460, 462, 518, 597, 616, 820, 1498, 2102, 2330, 2340, 2616, 2683, 2798]\n",
      "the list of indexs of word in phrase   [28, 13, 11, 5, 12, 7, 4, 15, 11, 6, 11, 9, 11, 6, 7, 38, 19]\n"
     ]
    }
   ],
   "source": [
    "phrases = list(brown.tagged_sents(f))\n",
    "phrases_bonne_seq =[]\n",
    "#the index of the phrase in phrases\n",
    "indp = 0\n",
    "cnt = 0\n",
    "#the list of indexs of the phrase in phrases\n",
    "list_indp = []\n",
    "#the list of indexs of word in phrase\n",
    "list_ind = []\n",
    "for phrase in phrases:\n",
    "    for word,tag in phrase:\n",
    "        if (word == \"that\"and tag == \"CS\"):\n",
    "            #index of the word in the phrase\n",
    "            ind = phrase.index((word,tag))\n",
    "            word1,tag =phrase[ind-1]\n",
    "            if tag.startswith(\"N\"):\n",
    "                word2,tag =phrase[ind-2]\n",
    "                if word2 ==\"the\":\n",
    "                    if phrase not in phrases_bonne_seq:\n",
    "                        phrases_bonne_seq.append(phrase)\n",
    "                        cnt = cnt + 1 \n",
    "                        list_indp.append(indp)\n",
    "                        list_ind.append(ind)\n",
    "    indp=indp+1\n",
    "\n",
    "print(\"number of phrases with good sequence :\",len(phrases_bonne_seq))\n",
    "print(\"the list of indexs of the phrase in phrases  \",list_indp)\n",
    "print(\"the list of indexs of word in phrase  \",list_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually detect if the phrase contains THAT  RELATIVE or THAT COMPLETIVE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "One , by Sen. Louis Crump of San Saba , would aid more than 17,000 retailers who pay a group of miscellaneous excise taxes by eliminating the requirement that each return be notarized . \n",
      "*********************************************\n",
      "2\n",
      "\n",
      "A certain vagueness may also be caused by tactical appreciation of the fact that the present council meeting is a semipublic affair , with no fewer than six Soviet correspondents accredited . \n",
      "*********************************************\n",
      "3\n",
      "\n",
      "His sense of urgency in this matter stems from the fact that court cases and juvenile arrests have more than doubled since 1948 , each year showing an increase in offenders . \n",
      "*********************************************\n",
      "4\n",
      "\n",
      "The council advised the governor that `` large supermarkets , factory outlets and department stores not be allowed to do business '' on Sunday . \n",
      "*********************************************\n",
      "5\n",
      "\n",
      "Mr. Martinelli has , in recent weeks , been of the opinion that a special town meeting would be called for the vote , while Mr. Bourcier said that a special election might be called instead . \n",
      "*********************************************\n",
      "6\n",
      "\n",
      "He assured Mr. Martinelli and the council that he would study the correct method and report back to the council as soon as possible . \n",
      "*********************************************\n",
      "7\n",
      "\n",
      "Sandman told the gathering that reports from workers on a local level all over the state indicate that Jones will be chosen the Republican Party's nominee with the largest majority given a candidate in recent years . \n",
      "*********************************************\n",
      "8\n",
      "\n",
      "The Mayor declined in two interviews with reporters yesterday to confirm or deny the reports that he had decided to run and wanted Mr. Screvane , who lives in Queens , to replace Abe Stark , the incumbent , as the candidate for President of the City Council and Mr. Beame , who lives in Brooklyn , to replace Mr. Gerosa as the candidate for Controller . \n",
      "*********************************************\n",
      "9\n",
      "\n",
      "A reader of the Boston newspapers can hardly escape the impression that petty chicanery , or worse , is the norm in Massachusetts public life . \n",
      "*********************************************\n",
      "10\n",
      "\n",
      "`` I am taking the position that the contract was clearly violated '' , Berger said . \n",
      "*********************************************\n",
      "11\n",
      "\n",
      "Perhaps the Pirate who will be the unhappiest over the news that Musial probably will sit out most of the series is Bob Friend , who was beaten by The Man twice last season on dramatic home runs . \n",
      "*********************************************\n",
      "12\n",
      "\n",
      "It was when he attempted to end the relationship that the murder took place . \n",
      "*********************************************\n",
      "13\n",
      "\n",
      "William Smythe , director of field service , told the commissioners that Multnomah , as of Aug. 22 , had spent $58,918 out of its budgeted $66,000 in the category , leaving only $7,082 for the rest of the month . \n",
      "*********************************************\n",
      "14\n",
      "\n",
      "He pointed out to the commissioners that the agency was literally dependent now on the machine processing , `` and the whole wheels of the agency would stop if it broke down or the three or four persons directing it were to leave '' . \n",
      "*********************************************\n",
      "15\n",
      "\n",
      "Lee said he had told the jury that he made an agreement in April with Stein to supply and supervise janitors in McCormick Place . \n",
      "*********************************************\n",
      "16\n",
      "\n",
      "Now , if Morton's newest product , a corn chip known as Chip-o's , turns out to sell as well as its stock did , the stock may turn out to be worth every cent of the prices that the avid buyers bid it up to . \n",
      "*********************************************\n",
      "17\n",
      "\n",
      "He also used as an example the manufacturer who introduced an all-automatic camera in Germany , with the result that it became the best selling camera in the German market . \n",
      "*********************************************\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for phrase in phrases_bonne_seq :\n",
    "    print(i)\n",
    "    print()\n",
    "\n",
    "    for word, tag in phrase:\n",
    "        print(word,\"\", end = '')\n",
    "    print()\n",
    "    i = i+1\n",
    "    print(\"*********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CST', 'CST', 'CST', 'CST', 'CST', 'CST', 'CST', 'WPR', 'WPR', 'CST', 'CST', 'WPR', 'CST', 'CST', 'CST', 'WPR', 'CST']\n"
     ]
    }
   ],
   "source": [
    "# R : 8 9 12 16\n",
    "#---\n",
    "# C :  1 , 2 , 3 , 4 , 5  , 6 , 7 , 10, 11, 13, 14, 15, 17\n",
    "r_or_c = []\n",
    "r_or_c.append(\"CST\")\n",
    "r_or_c.append(\"CST\")\n",
    "r_or_c.append(\"CST\")\n",
    "r_or_c.append(\"CST\")\n",
    "r_or_c.append(\"CST\")\n",
    "r_or_c.append(\"CST\")\n",
    "r_or_c.append(\"CST\")\n",
    "r_or_c.append(\"WPR\")\n",
    "r_or_c.append(\"WPR\")\n",
    "r_or_c.append(\"CST\")\n",
    "r_or_c.append(\"CST\")\n",
    "r_or_c.append(\"WPR\")\n",
    "r_or_c.append(\"CST\")\n",
    "r_or_c.append(\"CST\")\n",
    "r_or_c.append(\"CST\")\n",
    "r_or_c.append(\"WPR\")\n",
    "r_or_c.append(\"CST\")\n",
    "print(r_or_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changer tagged sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('that', 'CS')\n",
      "('that', 'CST')\n",
      "-----------------------------------\n",
      "('that', 'CS')\n",
      "('that', 'CST')\n",
      "-----------------------------------\n",
      "('that', 'CS')\n",
      "('that', 'CST')\n",
      "-----------------------------------\n",
      "('that', 'CS')\n",
      "('that', 'CST')\n",
      "-----------------------------------\n",
      "('that', 'CS')\n",
      "('that', 'CST')\n",
      "-----------------------------------\n",
      "('that', 'CS')\n",
      "('that', 'CST')\n",
      "-----------------------------------\n",
      "('that', 'CS')\n",
      "('that', 'CST')\n",
      "-----------------------------------\n",
      "('that', 'CS')\n",
      "('that', 'WPR')\n",
      "-----------------------------------\n",
      "('that', 'CS')\n",
      "('that', 'WPR')\n",
      "-----------------------------------\n",
      "('that', 'CS')\n",
      "('that', 'CST')\n",
      "-----------------------------------\n",
      "('that', 'CS')\n",
      "('that', 'CST')\n",
      "-----------------------------------\n",
      "('that', 'CS')\n",
      "('that', 'WPR')\n",
      "-----------------------------------\n",
      "('that', 'CS')\n",
      "('that', 'CST')\n",
      "-----------------------------------\n",
      "('that', 'CS')\n",
      "('that', 'CST')\n",
      "-----------------------------------\n",
      "('that', 'CS')\n",
      "('that', 'CST')\n",
      "-----------------------------------\n",
      "('that', 'CS')\n",
      "('that', 'WPR')\n",
      "-----------------------------------\n",
      "('that', 'CS')\n",
      "('that', 'CST')\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "word = \"\"\n",
    "tag = \"\"\n",
    "for i in range(0,17):\n",
    "    print(phrases[list_indp[i] ][list_ind[i]])\n",
    "    word, tag = phrases[list_indp[i]][list_ind[i]]\n",
    "    new_tag = r_or_c[i]\n",
    "    phrases[list_indp[i]][list_ind[i]] = (word,new_tag)\n",
    "    print(phrases[list_indp[i] ][list_ind[i]])\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now the tagget sentece are ready to be used in training \n",
    "Exemple : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('His', 'PP$'),\n",
       " ('sense', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('urgency', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('matter', 'NN'),\n",
       " ('stems', 'VBZ'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('fact', 'NN'),\n",
       " ('that', 'CST'),\n",
       " ('court', 'NN'),\n",
       " ('cases', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('juvenile', 'JJ'),\n",
       " ('arrests', 'NNS'),\n",
       " ('have', 'HV'),\n",
       " ('more', 'AP'),\n",
       " ('than', 'IN'),\n",
       " ('doubled', 'VBN'),\n",
       " ('since', 'IN'),\n",
       " ('1948', 'CD'),\n",
       " (',', ','),\n",
       " ('each', 'DT'),\n",
       " ('year', 'NN'),\n",
       " ('showing', 'VBG'),\n",
       " ('an', 'AT'),\n",
       " ('increase', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('offenders', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases[list_indp[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------Help--------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/****************************************************************************/\n",
    "/* How to use the TreeTagger                                                */\n",
    "/*                                                                          */\n",
    "/* Author: Helmut Schmid, IMS, University of Stuttgart, Germany             */\n",
    "/****************************************************************************/\n",
    "\n",
    "\n",
    "The TreeTagger consists of two programs: the training program creates\n",
    "a parameter file from a fullform lexicon and a handtagged corpus. The\n",
    "tagger program reads the parameter file and annotates the text with\n",
    "part of speech and lemma information. Both programs print information\n",
    "about their usage when they are called without arguments.\n",
    "\n",
    "\n",
    "Tagging\n",
    "-------\n",
    "\n",
    "Tagging is done with the *tree-tagger* program. \n",
    "\n",
    "The first argument is the name of a parameter file which was generated\n",
    "with the train-tree-tagger program. Parameter files generated on\n",
    "different platforms or with older versions of train-tree-tagger will\n",
    "not work.\n",
    "\n",
    "The second argument is the input file. It must be in one-word-per-line\n",
    "format, i.e. each line contains one token (word, punctuation character\n",
    "or parenthesis). Tokens may contain blanks. It is possible to override\n",
    "the lexical information contained in the parameter file of the tagger\n",
    "by specifying a list of possible tags after the token. This list has\n",
    "to be preceded by a tab character and the elements are separated by\n",
    "tab characters. Pretagging could be used e.g. to ensure that certain\n",
    "text-specific expressions are tagged correctly. Clitics (like \"'s\",\n",
    "\"'re\", and \"'d\" in English or \"-la\" and \"-t-elle\" in French) have to\n",
    "be separated if they were separated in the training data. (The French\n",
    "and English parameter files available by ftp expect separation of\n",
    "clitics).\n",
    "\n",
    "Sample input file:\n",
    "He\n",
    "moved\n",
    "to\n",
    "New York City\tNP\n",
    ".\n",
    "\n",
    "\n",
    "The third argument is the name of the output file. The output is also\n",
    "in one-word-per-line format. Depending on the specified options, it\n",
    "will contain columns with tokens, tags and lemmas. If the third\n",
    "argument is missing, the output will be printed to standard output. If\n",
    "the second argument is missing, too, input is read from standard\n",
    "input.\n",
    "\n",
    "Options:\n",
    "\n",
    "-token: Prints the token as well.\n",
    "-lemma: Prints the lemma as well.\n",
    "-sgml:  Don't tag SGML annotations, i.e. lines starting with '<' and ending\n",
    "        with '>'.\n",
    "-threshold <p>: Print all tags with a probability higher than <p> times the\n",
    "        probability of the best tag.\n",
    "-prob:  Print tag probabilities (requires option -threshold)\n",
    "-no-unknown: Print the token rather than <unknown> for unknown lemmas\n",
    "-quiet: Don't print status messages\n",
    "-pt-with-lemma: If this option is specified, then each pretagging tag\n",
    "        (see above) has to be followed by a whitespace and a lemma.\n",
    "-pt-with-prob: If this option is specified, then each pretagging tag\n",
    "        (see above) has to be followed by whitespace and a tag probability\n",
    "        value. If -pt-with-prob and -pt-with-lemma have been specified,\n",
    "        then each pretagging tag is followed by a probability and a lemma\n",
    "        in that order.\n",
    "-files f: Read the names of input and output files pairwise from the\n",
    "        file f. The format of f is the lexicon file format described below.\n",
    "-lex f: Read auxiliary lexicon entries from the file f.\n",
    "-eos-tag <tag>: The SGML tag <tag> signals the end of a sentence.\n",
    "        This option implies the option -sgml\n",
    "\n",
    "Some more exotic options:\n",
    "-proto: Print lexical information for each word\n",
    "  The lexicon type is signalled by one of the characters\n",
    "  f: The word was found in the full form lexicon.\n",
    "  c: The word in lowercase was found in the lexicon\n",
    "  h: The word contains an hyphen and the word following the hyphen was found\n",
    "     in the full form lexicon; e.g. instead of \"table-wine\" only \"wine\" has\n",
    "     been found.\n",
    "  s: The word has been looked up in the suffix lexicon\n",
    "  p: Tags have been assigned by pretagging.\n",
    "-gramotron: Same as -proto but with a different format\n",
    "-proto-with-prob: Same as -proto but with lexical tag probabilities\n",
    "-print-prob-tree: Print the transition probability tree and exit\n",
    "-eps <epsilon>: Value which is used to replace zero lexical frequencies.\n",
    "  Zero frequencies occur when a word/tag pair is contained in the lexicon\n",
    "  but not in the training corpus. The default is 0.1.\n",
    "-base:  Use only lexical probabilities for tagging. This option is only\n",
    "  useful to obtain a baseline result to which the actual tagger output is\n",
    "  compared.\n",
    "\n",
    "\n",
    "\n",
    "Training\n",
    "--------\n",
    "\n",
    "Training is done with the *train-tree-tagger* program. If the program is \n",
    "called without arguments, the following output is printed:\n",
    "\n",
    "USAGE: train-tree-tagger <lexicon> <open class file> <infile> <outfile> \n",
    "       {-cl <context length>} {-dtg <min. decision tree gain>}\n",
    "       {-ecw <eq. class weight>} {-atg <affix tree gain>} {-st <sent. tag>}\n",
    "\n",
    "Description of the command line arguments:\n",
    "* <lexicon>: name of a file which contains the fullform lexicon. Each line \n",
    "  of the lexicon corresponds to one word form and contains the word form \n",
    "  itself followed by a Tab character and a sequence of tag-lemma pairs.\n",
    "  The tags and lemmata are separated by whitespace.\n",
    "\n",
    "Example:\n",
    "aback\tRB aback\n",
    "abacuses\tNNS abacus\n",
    "abandon\tVB abandon\tVBP abandon\n",
    "abandoned\tJJ abandoned\tVBD abandon\tVBN abandon\n",
    "abandoning\tVBG abandon\n",
    "\n",
    "  Important: Ordinal and cardinal numbers which consist of digits\n",
    "  should not be included in the lexicon. Otherwise, the tagger will\n",
    "  not be able to learn how to tag numbers which are not listed in the\n",
    "  lexicon. Numbers with unusual tags should be added to the lexicon,\n",
    "  however.\n",
    "\n",
    "  Remark: The tagger doesn't need the lemmata for tagging. If\n",
    "  you do not have the lemma information or if you do not plan to\n",
    "  annotate corpora with lemmas, you can replace the lemma with a dummy\n",
    "  value, e.g. \"-\".\n",
    "\n",
    "* <open class file>: name of a file which contains a list of open class tags\n",
    "  i.e. possible tags of unknown word forms. This information is needed to\n",
    "  estimate likely tags of unknown words. This file would typically contain\n",
    "  adverb, adjective, noun, proper name and perhaps verb tags, but not\n",
    "  prepositions, determiners, pronouns or numbers.\n",
    "* <input file>: name of a file which contains tagged training data. The data\n",
    "  must be in one-word-per-line format. This means that each line contains \n",
    "  one token and one tag in that order separated by a tabulator. \n",
    "  Punctuation marks are considered as tokens and must have been tagged as well.\n",
    "\n",
    "Example:\n",
    "Pierre\tNP\n",
    "Vinken\tNP\n",
    ",\t,\n",
    "61\tCD\n",
    "years\tNNS\n",
    "\n",
    "* <output file>: name of the file in which the resulting tagger parameters \n",
    "  are stored.\n",
    "\n",
    "\n",
    "The following parameters are optional:\n",
    "\n",
    "* -cl <context length>: number of preceding words forming the tagging\n",
    "  context. The default is 2 which corresponds to a trigram context. For\n",
    "  small training corpora and/or large tagsets, it could be useful to reduce\n",
    "  this parameter to 1.\n",
    "* -dtg <min. decision tree gain>: Threshold - If the information gain at a \n",
    "  leaf node of the decision tree is below this threshold, the node is deleted.\n",
    "  The default value is 0.7.\n",
    "* -ecw <eq. class weight>: weight of the equivalence class based probability\n",
    "  estimates. The default is 0.15.\n",
    "* -atg <affix tree gain> Threshold - If the information gain at a leaf of an\n",
    "  affix tree is below this threshold, it is deleted. The default is 1.2.\n",
    "* -st <sent. tag>: the end-of-sentence part-of-speech tag, i.e. the tag which\n",
    "  is assigned to sentence punctuation like \".\", \"!\", \"?\". \n",
    "  Default is \"SENT\". It is important to set this option properly, if your\n",
    "  tag for sentence punctuation is not \"SENT\".\n",
    "\n",
    "The accuracy of the TreeTagger usually improves a bit, if different\n",
    "settings of the above parameters are tested and the best combination\n",
    "is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create lexicon file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "#for the LEMMA\n",
    "import spacy \n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_lg\")\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the LEMMA\n",
    "def spacy_process(texte):\n",
    "    tmp = []\n",
    "    tmp.append(texte)\n",
    "    for lt in tmp:\n",
    "        mytokens = nlp(lt)\n",
    "        mytokens2 = [word.lemma_.lower().strip() for word in mytokens]\n",
    "        mytokens2 = \" \".join([i for i in mytokens2])\n",
    "    return mytokens2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple sur LEMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test lemma :  hello i am talking to you\n",
      "Test lemma :  hello i be talk to you\n"
     ]
    }
   ],
   "source": [
    "test = \"hello i am talking to you\"\n",
    "print(\"Test lemma : \",test)\n",
    "test = spacy_process(test)\n",
    "print(\"Test lemma : \",test)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Example of lexicon file:\n",
    "aback\tRB aback\n",
    "abacuses\tNNS abacus\n",
    "abandon\tVB abandon\tVBP abandon\n",
    "abandoned\tJJ abandoned\tVBD abandon\tVBN abandon\n",
    "abandoning\tVBG abandon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon= open(\".\\\\lexicon\\\\lexicon.txt\",\"w+\")\n",
    "for phrase in phrases:\n",
    "    for word,tag in phrase:\n",
    "        lemma = spacy_process(word)\n",
    "        lexicon.write(word+\"\\t\"+tag+\" \"+lemma+\"\\n\")\n",
    "lexicon.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vider le fichier lexicon\n",
    "#lexicon= open(\".\\\\lexicon\\\\lexicon.txt\",\"w+\")\n",
    "#lexicon.write(\"\")\n",
    "#lexicon.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input file (tagged training data)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Example:\n",
    "Pierre\tNP\n",
    "Vinken\tNP\n",
    ",\t,\n",
    "61\tCD\n",
    "years\tNNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file= open(\".\\\\input\\\\input_file.txt\",\"w+\")\n",
    "for phrase in phrases:\n",
    "    for word,tag in phrase:\n",
    "        input_file.write(word+\"\\t\"+tag+\"\\n\")\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vider le fichier lexicon\n",
    "#input_file= open(\".\\\\input\\\\input_file.txt\",\"w+\")\n",
    "#input_file.write(\"\")\n",
    "#input_file.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "USAGE: train-tree-tagger [options] <lexicon> <open class file> <input file> <output file>\n",
    "\n",
    "OPTIONS:\n",
    "-st  <t> use <t> as sentence marker tag rather than SENT\n",
    "-cl  <n> consider <n> preceding tags for tagging (Default 2)\n",
    "-dtg <f> use <f> as threshold for decision tree gain (Default 0.500000)\n",
    "-sw  <f> use <f> as smoothing weight (Default 1.000000)\n",
    "-ecw <f> use <f> as equivalence class weight (Default 0.150000)\n",
    "-atg <f> use <f> as threshold for affix tree gain (Default 1.200000)\n",
    "-lt  <f> use <f> as threshold probability for lexical entries (default 0.000000)\n",
    "-utf8:   assume uft8 input\n",
    "-quiet   quiet mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "train-tree-tagger -cl 2 -dtg 0.50 -sw 1.00 -ecw 0.15 -atg 1.20 .\\\\lexicon\\\\lexicon.txt .\\\\input\\\\input_file.txt .\\\\lexicon\\\\input_file.txt .\\\\output_file\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\n",
      "ERROR: Two lexicon entries for word form \"!\"!\n"
     ]
    }
   ],
   "source": [
    "! train-tree-tagger -st . .\\\\lexicon\\\\lexicon.txt .\\\\input\\\\input_file.txt .\\\\lexicon\\\\input_file.txt .\\\\output_file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "train-tree-tagger -cl 2 -dtg 0.50 -sw 1.00 -ecw 0.15 -atg 1.20 .\\\\lexicon\\\\lexicon.txt .\\\\input\\\\input_file.txt .\\\\lexicon\\\\input_file.txt .\\\\output_file\n",
    "\n",
    "\treading the lexicon ...\n",
    "\t\treading the tagset ...\n",
    "\t\treading the lemmas ...\n",
    "\t\treading the entries ...\n",
    "\t\tsorting the lexicon ...\n",
    "\n",
    "ERROR: Two lexicon entries for word form \"!\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above we can notice that we have duplicate entries for the word \"!\" , so we have to do performe prétraotment on our LEXICON file and erase the deplicate words\n",
    "\n",
    "we have to respect this format\n",
    "\n",
    "abandoned______JJ_______abandoned________VBD______abandon_________VBN abandon\n",
    "\n",
    "instead of ysing this\n",
    "\n",
    "abandoned____JJ____abandoned\t\n",
    "abandoned____VBD____abandon\t\n",
    "abandoned____VBN____abandon\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple for the developement of the fuction for the detection of the duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrases_\n",
      "[['word0', 'tag', 'lemma'], ['word0', 'tag', 'lemma'], ['word0', 'tag', 'lemma'], ['word0', 'tagalpha', 'lemma1'], ['word0', 'tagalpha', 'lemma'], ['word0', 'tagalpha', 'lemma'], ['word0', 'tagalpha', 'lemma'], ['word0', 'tagalpha', 'lemma'], ['word0', 'tag', 'lemma'], ['word0', 'tag', 'lemma0'], ['word1', 'tag', 'lemma1'], ['word2', 'tag', 'lemma2'], ['word3', 'tag', 'lemma3'], ['word4', 'tag', 'lemma4']]\n",
      "\n",
      "phrases_no_duplication\n",
      "[['word0', 'tag', 'word0', 'tagalpha', 'lemma'], ['word0', 'tagalpha', 'word1', 'tag', 'lemma1'], ['word0', 'tag', 'lemma0'], ['word2', 'tag', 'lemma2'], ['word3', 'tag', 'lemma3'], ['word4', 'tag', 'lemma4']]\n"
     ]
    }
   ],
   "source": [
    "phrases_ = []\n",
    "phrases_.append([\"word0\",\"tag\",\"lemma\"])\n",
    "phrases_.append([\"word0\",\"tag\",\"lemma\"])\n",
    "phrases_.append([\"word0\",\"tag\",\"lemma\"])\n",
    "phrases_.append([\"word0\",\"tagalpha\",\"lemma1\"])\n",
    "phrases_.append([\"word0\",\"tagalpha\",\"lemma\"])\n",
    "phrases_.append([\"word0\",\"tagalpha\",\"lemma\"])\n",
    "\n",
    "phrases_.append([\"word0\",\"tagalpha\",\"lemma\"])\n",
    "\n",
    "phrases_.append([\"word0\",\"tagalpha\",\"lemma\"])\n",
    "\n",
    "phrases_.append([\"word0\",\"tag\",\"lemma\"])\n",
    "\n",
    "for i in range(0,5):\n",
    "    phrases_.append([\"word\"+str(i),\"tag\",\"lemma\"+str(i)])\n",
    "print(\"phrases_\")\n",
    "print(phrases_)\n",
    "print()\n",
    "\n",
    "phrases_no_duplication = []\n",
    "\n",
    "for token in phrases_:\n",
    "    length = len(phrases_no_duplication)\n",
    "    cnt=0\n",
    "    for phrase_no_duplication in phrases_no_duplication:\n",
    "        if token[-1] == phrase_no_duplication[-1]:\n",
    "            if token[1] not in phrase_no_duplication :\n",
    "                #word\n",
    "                lemma = phrase_no_duplication[-1]\n",
    "                del phrase_no_duplication[-1]\n",
    "                phrase_no_duplication.append(token[0])\n",
    "                #tag\n",
    "                phrase_no_duplication.append(token[1])\n",
    "                #lemma\n",
    "                phrase_no_duplication.append(lemma)\n",
    "                \n",
    "        else:\n",
    "                cnt = cnt+1\n",
    "    if length == cnt:\n",
    "        phrases_no_duplication.append(token)\n",
    "        \n",
    "print(\"phrases_no_duplication\")\n",
    "print(phrases_no_duplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare our Tokens in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'AT', 'the']\n",
      "['Fulton', 'NP-TL', 'fulton']\n",
      "['County', 'NN-TL', 'county']\n",
      "['Grand', 'JJ-TL', 'grand']\n",
      "['Jury', 'NN-TL', 'jury']\n"
     ]
    }
   ],
   "source": [
    "word_tagged_list = []\n",
    "for phrase in phrases:\n",
    "    for word,tag in phrase:\n",
    "        lemma = spacy_process(word)\n",
    "        word_tagged_list.append([word,tag,lemma])\n",
    "\n",
    "for i in range(0,5):\n",
    "    print(word_tagged_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implimentation of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tagged_list_no_duplication = []\n",
    "\n",
    "for token in word_tagged_list:\n",
    "    length = len(word_tagged_list_no_duplication)\n",
    "    cnt=0\n",
    "    for word_tagged_no_duplication in word_tagged_list_no_duplication:\n",
    "        if token[-1] == word_tagged_no_duplication[-1]:\n",
    "            if token[1] not in word_tagged_no_duplication :\n",
    "                \n",
    "                lemma = word_tagged_no_duplication[-1]\n",
    "                del word_tagged_no_duplication[-1]\n",
    "                #word\n",
    "                word_tagged_no_duplication.append(token[0])\n",
    "                #tag\n",
    "                word_tagged_no_duplication.append(token[1])\n",
    "                #lemma\n",
    "                word_tagged_no_duplication.append(lemma)\n",
    "\n",
    "        else:\n",
    "                cnt = cnt+1\n",
    "    if length == cnt:\n",
    "        word_tagged_list_no_duplication.append(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'AT', 'The', 'AT-TL', 'the', 'AT-HL', 'the']\n",
      "['Fulton', 'NP-TL', 'Fulton', 'NP', 'fulton']\n",
      "['County', 'NN-TL', 'counties', 'NNS', 'county', 'NN', 'Counties', 'NNS-TL', 'county']\n",
      "['Grand', 'JJ-TL', 'grand', 'JJ', 'grand']\n",
      "['Jury', 'NN-TL', 'jury', 'NN', 'juries', 'NNS', 'jury', 'NN-HL', 'jury']\n",
      "['said', 'VBD', 'says', 'VBZ', 'say', 'VB', 'said', 'VBN', 'saying', 'VBG', 'say']\n",
      "['Friday', 'NR', 'friday']\n",
      "['an', 'AT', 'an']\n",
      "['investigation', 'NN', 'Investigation', 'NN-TL', 'investigations', 'NNS', 'investigation']\n",
      "['of', 'IN', 'of', 'IN-TL', 'Of', 'IN-HL', 'of']\n",
      "[\"Atlanta's\", 'NP$', \"atlanta 's\"]\n",
      "['recent', 'JJ', 'recent']\n",
      "['primary', 'NN', 'primary', 'JJ', 'primary']\n",
      "['election', 'NN', 'elections', 'NNS', 'election']\n",
      "['produced', 'VBD', 'produce', 'VB', 'produce']\n",
      "['``', '``', '` `']\n",
      "['no', 'AT', 'no', 'RB', 'No', 'AT-HL', 'No', 'AT-TL', 'no']\n",
      "['evidence', 'NN', 'evidence']\n",
      "[\"''\", \"''\", \"''\"]\n",
      "['that', 'CS', 'that', 'WPS', 'that', 'CST', 'that', 'DT', 'that', 'QL', 'that', 'WPR', 'that']\n",
      "['any', 'DTI', 'any']\n",
      "['irregularities', 'NNS', 'irregularity']\n",
      "['took', 'VBD', 'take', 'VB', 'takes', 'VBZ', 'taken', 'VBN', 'taking', 'VBG', 'take']\n",
      "['place', 'NN', 'place', 'VB', 'places', 'NNS', 'placing', 'VBG', 'placed', 'VBN', 'Place', 'NN-TL', 'place']\n",
      "['.', '.', '.', '.-HL', '.']\n",
      "['further', 'RBR', 'further', 'JJR', 'far', 'RB', 'far', 'QL', 'far', 'JJ', 'further', 'VB', 'far']\n",
      "['in', 'IN', 'in', 'RP', 'in', 'IN-HL', 'in', 'IN-TL', 'in']\n",
      "['term-end', 'NN', 'term - end']\n",
      "['presentments', 'NNS', 'presentment']\n",
      "['City', 'NN-TL', 'city', 'NN', 'cities', 'NNS', 'City', 'NN-TL-HL', 'city']\n",
      "['Executive', 'JJ-TL', 'Executive', 'NN-TL', 'executive', 'NN', 'executives', 'NNS', 'Executives', 'NNS-TL', 'executive']\n",
      "['Committee', 'NN-TL', 'committee', 'NN', 'committees', 'NNS', 'committee']\n",
      "[',', ',', ',', ',-HL', ',']\n",
      "['which', 'WDT', 'which']\n",
      "['had', 'HVD', 'have', 'HV', 'has', 'HVZ', 'had', 'HVN', 'having', 'HVG', 'had', 'HVD-HL', 'have']\n",
      "['over-all', 'JJ', 'over - all']\n",
      "['charge', 'NN', 'charged', 'VBN', 'charged', 'VBD', 'charge', 'VB', 'charges', 'NNS', 'charged', 'VBN-HL', 'charges', 'VBZ', 'Charges', 'NNS-HL', 'charging', 'VBG', 'Charge', 'NN-HL', 'charge']\n",
      "['deserves', 'VBZ', 'deserve']\n",
      "['praise', 'NN', 'praised', 'VBD', 'praise']\n",
      "['and', 'CC', 'and', 'CC-TL', 'and', 'CC-HL', 'and']\n",
      "['thanks', 'NNS', 'thank', 'VB', 'thanks', 'NNS-HL', 'thank']\n",
      "['Atlanta', 'NP-TL', 'Atlanta', 'NP', 'atlanta']\n",
      "['for', 'IN', 'for', 'IN-TL', 'for', 'IN-HL', 'for', 'CS', 'for', 'RB', 'for']\n",
      "['manner', 'NN', 'manner']\n",
      "['was', 'BEDZ', 'been', 'BEN', 'are', 'BER', 'be', 'BE', 'is', 'BEZ', 'were', 'BED', 'being', 'BEG', 'are', 'BER-HL', 'am', 'BEM', 'is', 'BEZ-HL', 'be', 'BE-HL', 'was', 'BEDZ-HL', 'Are', 'BER-TL', 'be']\n",
      "['conducted', 'VBN', 'conduct', 'NN', 'conduct']\n",
      "['September-October', 'NP', 'september - october']\n",
      "['term', 'NN', 'termed', 'VBD', 'termed', 'VBN', 'terms', 'NNS', 'termed', 'VBN-HL', 'term']\n",
      "['by', 'IN', 'by', 'IN-HL', 'by']\n",
      "['Superior', 'JJ-TL', 'superior', 'JJ', 'superior']\n",
      "['Court', 'NN-TL', 'court', 'NN', 'courts', 'NNS', 'Court', 'NN-HL', 'court']\n",
      "['Judge', 'NN-TL', 'judges', 'NNS', 'judge', 'NN', 'judge', 'VB', 'judged', 'VBN', 'judge']\n",
      "['Durwood', 'NP', 'durwood']\n",
      "['Pye', 'NP', 'pye']\n",
      "['to', 'TO', 'to', 'IN', 'to', 'IN-HL', 'to', 'TO-HL', 'to', 'IN-TL', 'To', 'TO-TL', 'to', 'NPS', 'to']\n",
      "['investigate', 'VB', 'investigating', 'VBG', 'investigated', 'VBD', 'investigate']\n",
      "['reports', 'NNS', 'report', 'VB', 'report', 'NN', 'reported', 'VBD', 'reported', 'VBN', 'reported', 'VBN-HL', 'reporting', 'VBG', 'Report', 'VB-HL', 'reports', 'VBZ', 'report']\n",
      "['possible', 'JJ', 'possible']\n",
      "['hard-fought', 'JJ', 'hard - fight']\n",
      "['won', 'VBN', 'win', 'VB', 'winning', 'VBG', 'won', 'VBD', 'win', 'NN-HL', 'wins', 'VBZ', 'win']\n",
      "['Mayor-nominate', 'NN-TL', 'mayor - nominate']\n",
      "['Ivan', 'NP', 'ivan']\n",
      "['Allen', 'NP', 'allen']\n",
      "['Jr.', 'NP', 'jr.']\n",
      "['Only', 'RB', 'only', 'AP', 'only', 'JJ', 'only']\n",
      "['a', 'AT', 'A', 'AT-HL', 'A', 'NN', 'A', 'AT-TL', 'a']\n",
      "['relative', 'JJ', 'relatives', 'NNS', 'relative']\n",
      "['handful', 'NN', 'handful']\n",
      "['such', 'JJ', 'such', 'ABL', 'such', 'QL', 'such']\n",
      "['received', 'VBN', 'receives', 'VBZ', 'receive', 'VB', 'received', 'VBD', 'receiving', 'VBG', 'receive']\n",
      "['considering', 'IN', 'consider', 'VB', 'considered', 'VBD', 'considered', 'VBN', 'considering', 'VBG', 'consider']\n",
      "['widespread', 'JJ', 'widespread']\n",
      "['interest', 'NN', 'interest']\n",
      "['number', 'NN', 'number', 'NN-HL', 'numbers', 'NNS', 'number']\n",
      "['voters', 'NNS', 'voter', 'NN', 'voter']\n",
      "['size', 'NN', 'size']\n",
      "['this', 'DT', 'this', 'DT-HL', 'this']\n",
      "['it', 'PPS', 'it', 'PPO', 'it']\n",
      "['did', 'DOD', 'do', 'DO', 'done', 'VBN', 'does', 'DOZ', 'doing', 'VBG', 'do', 'DO-HL', 'do']\n",
      "['find', 'VB', 'found', 'VBD', 'found', 'VBN', 'finding', 'VBG', 'finds', 'VBZ', 'finding', 'NN', 'find']\n",
      "['many', 'AP', 'Many', 'AP-HL', 'many']\n",
      "[\"Georgia's\", 'NP$', \"georgia 's\"]\n",
      "['registration', 'NN', 'Registrations', 'NNS', 'registration']\n",
      "['laws', 'NNS', 'law', 'NN', 'Law', 'NN-TL', 'law']\n",
      "['outmoded', 'JJ', 'outmoded']\n",
      "['or', 'CC', 'or']\n",
      "['inadequate', 'JJ', 'inadequate']\n",
      "['often', 'RB', 'often']\n",
      "['ambiguous', 'JJ', 'ambiguous']\n",
      "['recommended', 'VBD', 'recommended', 'VBN', 'recommend', 'VB', 'recommends', 'VBZ', 'recommend']\n",
      "['legislators', 'NNS', 'legislator', 'NN', 'Legislators', 'NNS-HL', 'legislator']\n",
      "['act', 'VB', 'act', 'NN', 'acts', 'NNS', 'Acting', 'VBG-TL', 'Acting', 'VBG', 'Act', 'NN-TL', 'acted', 'VBD', 'act']\n",
      "['these', 'DTS', 'these']\n",
      "['studied', 'VBN', 'study', 'NN', 'studied', 'VBD', 'study', 'VB', 'studied', 'VBN-HL', 'studying', 'VBG', 'studies', 'NNS', 'study']\n",
      "['revised', 'VBN', 'revised', 'VBD', 'revise']\n",
      "['end', 'NN', 'ended', 'VBD', 'end', 'VB', 'ends', 'NNS', 'ending', 'VBG', 'ended', 'VBN', 'End', 'NN-TL', 'end']\n",
      "['modernizing', 'VBG', 'modernize']\n",
      "['improving', 'VBG', 'improve', 'VB', 'improved', 'VBN-HL', 'improved', 'VBN', 'improve']\n",
      "['them', 'PPO', 'they', 'PPSS', 'they']\n",
      "['commented', 'VBD', 'comment', 'VB', 'comments', 'NNS-HL', 'comments', 'NNS', 'comment', 'NN', 'commenting', 'VBG', 'comment']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,100):\n",
    "    print(word_tagged_list_no_duplication[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recrating the lexicon file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon= open(\".\\\\lexicon\\\\lexicon.txt\",\"w+\")\n",
    "for token in word_tagged_list_no_duplication:\n",
    "    lexicon.write(token[0]+\"\\t\"+token[1])\n",
    "    for i in range(2,len(token),1):\n",
    "        lexicon.write(\"\\t\"+token[i])\n",
    "    lexicon.write(\"\\n\")\n",
    "\n",
    "lexicon.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\\\input\\\\input_file.txt\",'r') as f1 :\n",
    "    lines = f1.readlines()\n",
    "    with open(\".\\\\class\\\\openclassfile.txt\",'w') as f2 :\n",
    "        tags = []\n",
    "        for line in lines :\n",
    "            tags.append(line.split('\\t')[1].strip())\n",
    "        tags = set(tags) #pour supprimer les doublons\n",
    "        for i in tags :\n",
    "            f2.write(i + \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "with open('NCC.test-utf8.txt','r') as f1 :\n",
    "    text = f1.read()\n",
    "    list_word = TreebankWordTokenizer().tokenize(text)\n",
    "    with open('test.txt','w') as f2 :\n",
    "        for word in list_word :\n",
    "            f2.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training TreeTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "train-tree-tagger -cl 2 -dtg 0.50 -sw 1.00 -ecw 0.15 -atg 1.20 .\\\\lexicon\\\\lexicon.txt .\\\\class\\\\openclassfile.txt .\\\\input\\\\input_file.txt .\\\\output_file.par\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\t\treading the open class tags ...\n",
      "\tcalculating tag frequencies ...\n",
      "\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\tmaking affix tree ...\n",
      "prefix lexicon: 1869 nodes\n",
      "suffix lexicon: 1318 nodes\n",
      "\treading classes ...\n",
      "\tmaking ngram table ...\n",
      "\n",
      "1000\t700\n",
      "2000\t1272\n",
      "3000\t1726\n",
      "4000\t2066\n",
      "5000\t2425\n",
      "6000\t2745\n",
      "7000\t3077\n",
      "8000\t3468\n",
      "9000\t3787\n",
      "10000\t4032\n",
      "11000\t4311\n",
      "12000\t4563\n",
      "13000\t4785\n",
      "14000\t5024\n",
      "15000\t5310\n",
      "16000\t5512\n",
      "17000\t5774\n",
      "18000\t5957\n",
      "19000\t6119\n",
      "20000\t6295\n",
      "21000\t6510\n",
      "22000\t6715\n",
      "23000\t6957\n",
      "24000\t7203\n",
      "25000\t7404\n",
      "26000\t7646\n",
      "27000\t7884\n",
      "28000\t8054\n",
      "29000\t8273\n",
      "30000\t8408\n",
      "31000\t8577\n",
      "32000\t8786\n",
      "33000\t8996\n",
      "34000\t9227\n",
      "35000\t9380\n",
      "36000\t9597\n",
      "37000\t9746\n",
      "38000\t9975\n",
      "39000\t10094\n",
      "40000\t10184\n",
      "41000\t10269\n",
      "42000\t10415\n",
      "43000\t10559\n",
      "44000\t10695\n",
      "45000\t10828\n",
      "46000\t10938\n",
      "47000\t11080\n",
      "48000\t11225\n",
      "49000\t11345\n",
      "50000\t11436\n",
      "51000\t11540\n",
      "52000\t11645\n",
      "53000\t11721\n",
      "54000\t11814\n",
      "55000\t11927\n",
      "56000\t12049\n",
      "57000\t12136\n",
      "58000\t12331\n",
      "59000\t12422\n",
      "60000\t12559\n",
      "61000\t12688\n",
      "62000\t12840\n",
      "63000\t12928\n",
      "64000\t13042\n",
      "65000\t13195\n",
      "66000\t13320\n",
      "67000\t13452\n",
      "68000\t13604\n",
      "68399\t13654\n",
      "finished.\n",
      "\tmaking decision tree ...\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\tsaving parameters ...\n",
      "\n",
      "Number of nodes: 351\n",
      "Max. path length: 70\n",
      "\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "! train-tree-tagger -st . .\\\\lexicon\\\\lexicon.txt .\\\\class\\\\openclassfile.txt .\\\\input\\\\input_file.txt  .\\\\output_file.par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "\t finished.\n"
     ]
    }
   ],
   "source": [
    "! tree-tagger.exe .\\\\output_file.par .\\\\test.txt .\\\\outputt.txt -token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
